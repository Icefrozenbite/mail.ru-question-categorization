{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\answers.jsonl\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\main_category_mapper.json\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\russian-dictionary-master.zip\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\sample_submission.csv\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\sub_category_mapper.json\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\test.csv\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\train.csv\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\unsupervised.csv\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\zaliznjak_forms.txt\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\russian-dictionary-master\\adjectives.csv\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\russian-dictionary-master\\LICENSE\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\russian-dictionary-master\\nouns.csv\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\russian-dictionary-master\\others.csv\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\russian-dictionary-master\\README.md\n",
      "C:\\\\NonSystem\\\\Work\\\\mail\\\\russian-dictionary-master\\verbs.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk(r'C:\\\\NonSystem\\\\Work\\\\mail\\\\'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'C:\\\\NonSystem\\\\Work\\\\mail\\\\train.csv')\n",
    "test = pd.read_csv(r'C:\\\\NonSystem\\\\Work\\\\mail\\\\test.csv')\n",
    "unsupervised = pd.read_csv(r'C:\\\\NonSystem\\\\Work\\\\mail\\\\unsupervised.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised.insert(0,'main_category',['' for i in range(len(unsupervised))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised.insert(1,'sub_category',['' for i in range(len(unsupervised))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary to map a Russian word with its lemma.\n",
    "lem_dict = {}\n",
    "with open(r'C:\\\\NonSystem\\\\Work\\\\mail\\\\zaliznjak_forms.txt',encoding='utf-8-sig') as dictionary:\n",
    "    dictionary = dictionary.read()\n",
    "    dictionary = dictionary.split('\\n')\n",
    "    for i in dictionary:\n",
    "        i = i.split(',')\n",
    "        try:\n",
    "            word = i[0].replace(\"'\", \"\")\n",
    "            lemma = i[1]\n",
    "            lem_dict[word] = lemma\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation. Reverse the sentences.\n",
    "def data_aug(dataframe):\n",
    "    \n",
    "    aug_train = {'main_category':[],'sub_category':[],'question':[]}\n",
    "    for index, content in dataframe.iterrows():\n",
    "        main = content['main_category']\n",
    "        sub = content['sub_category']\n",
    "        ques = content['question']\n",
    "        ques = ques.split(' ')\n",
    "        ques = ques[::-1]\n",
    "        ques = ' '.join(ques)\n",
    "        aug_train['main_category'].append(main)\n",
    "        aug_train['sub_category'].append(sub)\n",
    "        aug_train['question'].append(ques)\n",
    "    aug_train_pd = pd.DataFrame(aug_train)\n",
    "    new_dataframe = dataframe.append(aug_train_pd, ignore_index=True)\n",
    "    \n",
    "    return new_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_aug(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised = data_aug(unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \n",
    "    words = wordpunct_tokenize(text.lower())\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1333332/1333332 [00:13<00:00, 102518.67it/s]\n"
     ]
    }
   ],
   "source": [
    "word2freq = {}\n",
    "\n",
    "for question in tqdm(train.question):\n",
    "    \n",
    "    words = process_text(question)\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        if word in word2freq:\n",
    "            word2freq[word] += 1\n",
    "        else:\n",
    "            word2freq[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read word2vec: 100%|██████████████████████████████████████████████████████| 2000000/2000000 [01:12<00:00, 27544.24it/s]\n"
     ]
    }
   ],
   "source": [
    "word2index = {'PAD': 0}\n",
    "vectors = []\n",
    "    \n",
    "word2vec_file = open(r'C:\\Users\\gonge\\cc.ru.300.vec',encoding='utf-8')\n",
    "    \n",
    "n_words, embedding_dim = word2vec_file.readline().split()\n",
    "n_words, embedding_dim = int(n_words), int(embedding_dim)\n",
    "\n",
    "# Zero vector for PAD\n",
    "vectors.append(np.zeros((1, embedding_dim)))\n",
    "\n",
    "progress_bar = tqdm(desc='Read word2vec', total=n_words)\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    line = word2vec_file.readline().strip()\n",
    "\n",
    "    if not line:\n",
    "        break\n",
    "        \n",
    "    current_parts = line.split()\n",
    "\n",
    "    current_word = ' '.join(current_parts[:-embedding_dim])\n",
    "\n",
    "    if current_word in word2freq:\n",
    "\n",
    "        word2index[current_word] = len(word2index)\n",
    "\n",
    "        current_vectors = current_parts[-embedding_dim:]\n",
    "        current_vectors = np.array(list(map(float, current_vectors)))\n",
    "        current_vectors = np.expand_dims(current_vectors, 0)\n",
    "\n",
    "        vectors.append(current_vectors)\n",
    "\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "word2vec_file.close()\n",
    "\n",
    "vectors = np.concatenate(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мы не знаем 3.02 % слов в датасете\n",
      "Количество неизвестных слов 107889 из 328896, то есть 32.80 % уникальных слов в словаре\n",
      "В среднем каждое встречается 4.17 раз\n",
      "\n",
      "Топ 5 невошедших слов:\n",
      "??? с количеством вхождениий - 17208\n",
      "!!! с количеством вхождениий - 13952\n",
      "?) с количеством вхождениий - 13226\n",
      "?? с количеством вхождениий - 12654\n",
      "\"? с количеством вхождениий - 9162\n"
     ]
    }
   ],
   "source": [
    "unk_words = [word for word in word2freq if word not in word2index]\n",
    "unk_counts = [word2freq[word] for word in unk_words]\n",
    "n_unk = sum(unk_counts) * 100 / sum(list(word2freq.values()))\n",
    "\n",
    "sub_sample_unk_words = {word: word2freq[word] for word in unk_words}\n",
    "sorted_unk_words = list(sorted(sub_sample_unk_words, key=lambda x: sub_sample_unk_words[x], reverse=True))\n",
    "\n",
    "print('Мы не знаем {:.2f} % слов в датасете'.format(n_unk))\n",
    "print('Количество неизвестных слов {} из {}, то есть {:.2f} % уникальных слов в словаре'.format(\n",
    "    len(unk_words), len(word2freq), len(unk_words) * 100 / len(word2freq)))\n",
    "print('В среднем каждое встречается {:.2f} раз'.format(np.mean(unk_counts)))\n",
    "print()\n",
    "print('Топ 5 невошедших слов:')\n",
    "\n",
    "for i in range(5):\n",
    "    print(sorted_unk_words[i], 'с количеством вхождениий -', word2freq[sorted_unk_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordData(Dataset):\n",
    "    \n",
    "    def __init__(self, x_data, y_data, word2index, sequence_length=32, pad_token='PAD', verbose=True):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.x_data = []\n",
    "        self.y_data = y_data\n",
    "        \n",
    "        self.word2index = word2index\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        self.pad_index = self.word2index[self.pad_token]\n",
    "        \n",
    "        self.load(x_data, verbose=verbose)\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_text(text):\n",
    "        \n",
    "        # Место для вашей предобработки\n",
    "    \n",
    "        words = wordpunct_tokenize(text.lower())\n",
    "        words = [word for word in words if word.isalpha()] # We don't want non-word tokens.\n",
    "        #words = [lem_dict[word] for word if word in lem_dict] #For lemmatization, which worsens the performance. So we don't use it here.\n",
    "        \n",
    "        return words\n",
    "        \n",
    "    def load(self, data, verbose=True):\n",
    "        \n",
    "        data_iterator = tqdm(data, desc='Loading data', disable=not verbose)\n",
    "        \n",
    "        for text in data_iterator:\n",
    "            words = self.process_text(text)\n",
    "            indexed_words = self.indexing(words)\n",
    "            self.x_data.append(indexed_words)\n",
    "    \n",
    "    def indexing(self, tokenized_text):\n",
    "\n",
    "        # здесь мы не используем токен UNK, потому что мы мы его специально не учили\n",
    "        # становится непонятно какой же эмбеддинг присвоить неизвестному слову,\n",
    "        # поэтому просто выбрасываем наши неизветсные слова\n",
    "        \n",
    "        ### CODE ###\n",
    "\n",
    "        return [self.word2index[token] for token in tokenized_text if token in self.word2index ]\n",
    "    \n",
    "    def padding(self, sequence):\n",
    "        \n",
    "        # Ограничить длину self.sequence_length\n",
    "        # если длина меньше максимально - западить\n",
    "        \n",
    "        ### CODE ###\n",
    "\n",
    "        if len(sequence) > self.sequence_length:\n",
    "            sequence = sequence[:self.sequence_length]\n",
    "        elif len(sequence) < self.sequence_length:\n",
    "            sequence = sequence + [self.pad_index] * (self.sequence_length - len(sequence))\n",
    "\n",
    "        return sequence\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = self.x_data[idx]\n",
    "        x = self.padding(x)\n",
    "        x = torch.Tensor(x).long()\n",
    "        \n",
    "        y = self.y_data[idx]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|███████████████████████████████████████████████████████| 1199998/1199998 [00:13<00:00, 86738.38it/s]\n",
      "Loading data: 100%|███████████████████████████████████████████████████████| 1199998/1199998 [00:13<00:00, 87101.20it/s]\n",
      "Loading data: 100%|█████████████████████████████████████████████████████████| 133334/133334 [00:01<00:00, 91806.50it/s]\n",
      "Loading data: 100%|█████████████████████████████████████████████████████████| 133334/133334 [00:01<00:00, 67804.34it/s]\n",
      "Loading data: 100%|█████████████████████████████████████████████████████████| 200000/200000 [00:02<00:00, 93347.64it/s]\n",
      "Loading data: 100%|█████████████████████████████████████████████████████████| 200000/200000 [00:02<00:00, 93787.47it/s]\n"
     ]
    }
   ],
   "source": [
    "main_x_train, main_x_validation, main_y_train, main_y_validation = train_test_split(train.question, train.main_category, test_size=0.1)\n",
    "sub_x_train, sub_x_validation, sub_y_train, sub_y_validation = train_test_split(train.question, train.sub_category, test_size=0.1)\n",
    "\n",
    "main_train_dataset = WordData(list(main_x_train), list(main_y_train), word2index)\n",
    "main_train_loader = DataLoader(main_train_dataset, batch_size=64)\n",
    "\n",
    "sub_train_dataset = WordData(list(sub_x_train), list(sub_y_train), word2index)\n",
    "sub_train_loader = DataLoader(sub_train_dataset, batch_size=64)\n",
    "\n",
    "main_validation_dataset = WordData(list(main_x_validation), list(main_y_validation), word2index)\n",
    "main_validation_loader = DataLoader(main_validation_dataset, batch_size=64)\n",
    "\n",
    "sub_validation_dataset = WordData(list(sub_x_validation), list(sub_y_validation), word2index)\n",
    "sub_validation_loader = DataLoader(sub_validation_dataset, batch_size=64)\n",
    "\n",
    "main_test_dataset = WordData(list(test.question), np.zeros((test.shape[0])), word2index)\n",
    "main_test_loader = DataLoader(main_test_dataset, batch_size=64)\n",
    "\n",
    "sub_test_dataset = WordData(list(test.question), np.zeros((test.shape[0])), word2index)\n",
    "sub_test_loader = DataLoader(sub_test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 100%|███████████████████████████████████████████████████████| 5070835/5070835 [00:59<00:00, 84738.60it/s]\n",
      "Loading data: 100%|███████████████████████████████████████████████████████| 5070835/5070835 [00:58<00:00, 86983.32it/s]\n"
     ]
    }
   ],
   "source": [
    "main_x_unsu, main_x_unsu_vali, main_y_unsu, main_y_unsu_vali = train_test_split(unsupervised.question, unsupervised.main_category, test_size=0.00001)\n",
    "sub_x_unsu, sub_x_unsu_vali, sub_y_unsu, sub_y_unsu_vali = train_test_split(unsupervised.question, unsupervised.sub_category, test_size=0.00001)\n",
    "main_unsu_dataset = WordData(list(main_x_unsu), list(main_y_unsu), word2index)\n",
    "main_unsu_loader = DataLoader(main_unsu_dataset, batch_size=64)\n",
    "sub_unsu_dataset = WordData(list(sub_x_unsu), list(sub_y_unsu), word2index)\n",
    "sub_unsu_loader = DataLoader(sub_unsu_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAverageNetwork(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_matrix, n_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_layer = torch.nn.Embedding.from_pretrained(torch.Tensor(embedding_matrix))\n",
    "        \n",
    "        self.layers = torch.nn.Sequential(torch.nn.Linear(300, 256),\n",
    "                                          torch.nn.ReLU(), \n",
    "                                          torch.nn.Linear(256, 128),\n",
    "                                          torch.nn.ReLU(),\n",
    "                                          torch.nn.Linear(128, n_classes))\n",
    "    def forward(self, x):\n",
    "        \n",
    "        \n",
    "        \n",
    "        sequence_lengths = (x > 0).sum(dim=1)\n",
    "        sequence_lengths[sequence_lengths == 0.] = 1\n",
    "        \n",
    "        x = self.embedding_layer(x)\n",
    "        \n",
    "        x = x.mean(dim=-2)\n",
    "        \n",
    "        lengths_scaling = sequence_lengths.float() / x.size(1)\n",
    "        lengths_scaling = lengths_scaling.unsqueeze(1).repeat((1, x.size(-1)))\n",
    "        x /= lengths_scaling.to(x.device)\n",
    "        \n",
    "        x = self.layers(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 64\n",
    "main_num_classes = 28\n",
    "sub_num_classes = 209\n",
    "n_hidden = 128\n",
    "vocab_size = len(word2index)\n",
    "embedding_dim  = 512\n",
    "\n",
    "def attention(encode):\n",
    "\n",
    "        attention_scores = torch.bmm(encode, encode.transpose(1, 2))\n",
    "        attention_distribution = torch.softmax(attention_scores, 2)\n",
    "        attention_vectors = torch.bmm(attention_distribution, encode)\n",
    "\n",
    "        decoder_with_attention = torch.cat([encode, attention_vectors],dim=-1)\n",
    "        \n",
    "        return decoder_with_attention\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,embedding_dim, n_hidden):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.cnn = torch.nn.Conv1d(in_channels=300, out_channels=256, kernel_size=2)\n",
    "        self.linear = torch.nn.Linear(in_features=256, out_features=128)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        X = self.dropout(X)\n",
    "        X = X.transpose(0,1)\n",
    "        X, states = self.lstm(X)\n",
    "        X = X.transpose(0,1)\n",
    "        X = self.dropout(X)\n",
    "        X = attention(X)\n",
    "        X = X.transpose(1,2)\n",
    "        X = self.cnn(X)\n",
    "        X = torch.relu(X)\n",
    "        X = self.pool(X)\n",
    "        X = X.squeeze()\n",
    "        X = self.linear(X)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1/1 [03:43<00:00, 223.87s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a43117d27bdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[0mmain_test_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_test_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m \u001b[0mmain_chosen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_chosen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[0msub_chosen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_chosen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def sigmoid(d):\n",
    "    np.sum(np.exp(d)/np.sum(np.exp(d)) *np.array([0,1,2,3,4]))\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')    \n",
    "\n",
    "main_num_classes = 28\n",
    "sub_num_classes = 209\n",
    "main_mode = DeepAverageNetwork(embedding_matrix=vectors, n_classes=main_num_classes)\n",
    "#main_mode = Model().to(device)  # complicated model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "sub_mode = DeepAverageNetwork(embedding_matrix=vectors, n_classes=sub_num_classes)\n",
    "sub_mode = sub_mode.to(device)\n",
    "#sub_mode = Model().to(device)\n",
    "optimizer2 = optim.Adam(sub_mode.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "\n",
    "main_mode = main_mode.to(device)\n",
    "\n",
    "optimizer1 = optim.Adam(main_mode.parameters(), lr=0.01)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "# Training\n",
    "\n",
    "epochs = 10\n",
    "main_losses = []\n",
    "sub_losses = []\n",
    "best_test_loss = 10.\n",
    "\n",
    "main_test_f1, sub_test_f1 = [], []\n",
    "\n",
    "for n_epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    main_train_losses = []\n",
    "    main_test_losses = []\n",
    "    main_test_targets = []\n",
    "    main_test_pred_class = []\n",
    "    sub_train_losses = []\n",
    "    sub_test_losses = []\n",
    "    sub_test_targets = []\n",
    "    sub_test_pred_class = []\n",
    "    \n",
    "    sub_unsu_targets = []\n",
    "    sub_unsu_pred_class = []\n",
    "    main_unsu_targets = []\n",
    "    main_unsu_pred_class = []\n",
    "    main_chosen = []\n",
    "    sub_chosen = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    main_mode.train()\n",
    "    \n",
    "    batch_count = 1\n",
    "    \n",
    "    for x, y in main_train_loader:\n",
    "        \n",
    "        batch_count += 1\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer1.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred = main_mode(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        main_train_losses.append(loss.item())\n",
    "        main_losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        \n",
    "       \n",
    "    \n",
    "        \n",
    "    main_mode.eval()\n",
    "    \n",
    "    \n",
    "    for x, y in main_validation_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "           \n",
    "            pred = main_mode(x)\n",
    "            \n",
    "            pred = pred.cpu()\n",
    "            \n",
    "            y = y.cpu()\n",
    "\n",
    "            main_test_targets.append(y.numpy())\n",
    "            \n",
    "            main_test_pred_class.append(np.argmax(pred, axis=1))\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            main_test_losses.append(loss.item())\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    sub_mode.train()\n",
    "    \n",
    "    for x, y in sub_train_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred = sub_mode(x)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer2.step()\n",
    "        \n",
    "        sub_train_losses.append(loss.item())\n",
    "        sub_losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    sub_mode.eval()\n",
    "    \n",
    "    for x, y in sub_validation_loader:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred = sub_mode(x)\n",
    "\n",
    "            pred = pred.cpu()\n",
    "            y = y.cpu()\n",
    "\n",
    "            sub_test_targets.append(y.numpy())\n",
    "            sub_test_pred_class.append(np.argmax(pred, axis=1))\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            sub_test_losses.append(loss.item())\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "main_unsu_targets = list(main_unsu_targets)\n",
    "main_unsu_pred_class = list(main_unsu_pred_class)\n",
    "    \n",
    "for m, n in main_unsu_loader:\n",
    "    n = torch.Tensor(n)\n",
    "    m = m.to(device)\n",
    "    n = n.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred = main_mode(m)\n",
    "\n",
    "        pred = pred.cpu()\n",
    "        n = n.cpu()\n",
    "        m = m.cpu()    \n",
    "        \n",
    "        main_unsu_targets.append(pred.numpy())\n",
    "        main_unsu_pred_class.append(np.argmax(pred, axis=1))\n",
    "    \n",
    "        \n",
    "        \n",
    "main_unsu_targets = np.concatenate(main_unsu_targets).squeeze()\n",
    "main_unsu_pred_class = np.concatenate(main_unsu_pred_class).squeeze() \n",
    "  \n",
    "    \n",
    "sub_unsu_targets = list(sub_unsu_targets)\n",
    "sub_unsu_pred_class = list(sub_unsu_pred_class)\n",
    "    \n",
    "for m, n in sub_unsu_loader:\n",
    "    n = torch.Tensor(n)\n",
    "    m = m.to(device)\n",
    "    n = n.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pred = sub_mode(m)\n",
    "\n",
    "        pred = pred.cpu()        \n",
    "        n = n.cpu()\n",
    "        m = m.cpu()\n",
    "        sub_unsu_targets.append(pred.numpy())\n",
    "        sub_unsu_pred_class.append(np.argmax(pred, axis=1))\n",
    "        \n",
    "sub_unsu_targets = np.concatenate(sub_unsu_targets).squeeze()\n",
    "sub_unsu_pred_class = np.concatenate(sub_unsu_pred_class).squeeze()     \n",
    "        \n",
    "main_test_loss = np.mean(main_test_losses)\n",
    "main_test_targets = np.concatenate(main_test_targets).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "            \n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Main category losses: train - 1.801, test - 1.758\n",
      "Sub category losses: train - 3.423, test - 3.336\n",
      "F1 test - 0.432\n"
     ]
    }
   ],
   "source": [
    "main_test_pred_class = np.concatenate(main_test_pred_class).squeeze()\n",
    "main_f1 = f1_score(main_test_targets, main_test_pred_class, average='micro')\n",
    "main_test_f1.append(main_f1)\n",
    "sub_test_loss = np.mean(sub_test_losses)\n",
    "sub_test_targets = np.concatenate(sub_test_targets).squeeze()\n",
    "sub_test_pred_class = np.concatenate(sub_test_pred_class).squeeze()\n",
    "sub_f1 = f1_score(sub_test_targets, sub_test_pred_class, average='micro')\n",
    "sub_test_f1.append(sub_f1)\n",
    "    \n",
    "print()\n",
    "print('Main category losses: train - {:.3f}, test - {:.3f}'.format(np.mean(main_train_losses), main_test_loss))\n",
    "print('Sub category losses: train - {:.3f}, test - {:.3f}'.format(np.mean(sub_train_losses), sub_test_loss))\n",
    "print('F1 test - {:.3f}'.format(main_f1*0.7 + sub_f1*0.3))\n",
    "mean_test_loss = np.mean(main_test_losses)*0.7 + np.mean(sub_test_losses)*0.3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_y_unsu.update(pd.Series(list(main_unsu_pred_class)))\n",
    "sub_y_unsu.update(pd.Series(list(sub_unsu_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_main_x_train = main_x_train.append(main_x_unsu)\n",
    "new_main_y_train = main_y_train.append(main_y_unsu)\n",
    "new_sub_x_train = sub_x_train.append(sub_x_unsu)\n",
    "new_sub_y_train = sub_y_train.append(sub_y_unsu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_main_x = new_main_x_train\n",
    "full_main_y = new_main_y_train\n",
    "full_sub_x = new_sub_x_train\n",
    "full_sub_y = new_sub_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_full(full):\n",
    "    list_full_result = []\n",
    "    for i in full:\n",
    "        if i != '':\n",
    "            list_full_result.append(i)\n",
    "        else:\n",
    "            list_full_result.append(0)\n",
    "    \n",
    "    return list_full_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9e7f18effd54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist_full_main_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_main_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlist_full_sub_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_sub_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlist_full_main_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_main_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlist_full_sub_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_sub_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-8eb8b0671a46>\u001b[0m in \u001b[0;36mlist_full\u001b[1;34m(full)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0mlist_full_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mlist_full_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_full_main_y = list_full(full_main_y)\n",
    "list_full_sub_y = list_full(full_sub_y)\n",
    "list_full_main_x = list_full(full_main_x)\n",
    "list_full_sub_x = list_full(full_sub_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_main_x = pd.Series(list_full_main_x)\n",
    "full_main_y = pd.Series([i for i in list_full_main_y])\n",
    "full_sub_x = pd.Series(list_full_sub_x)\n",
    "full_sub_y = pd.Series([i for i in list_full_sub_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_main_x_test = full_main_x[int(len(full_main_x)*0.9):]\n",
    "new_main_y_test = full_main_y[int(len(full_main_y)*0.9):]\n",
    "new_sub_x_test = full_sub_x[int(len(full_sub_x)*0.9):]\n",
    "new_sub_y_test = full_sub_y[int(len(full_sub_y)*0.9):]\n",
    "\n",
    "new_main_x_train = full_main_x[:int(len(full_main_x)*0.9)]\n",
    "new_main_y_train = full_main_y[:int(len(full_main_y)*0.9)]\n",
    "new_sub_x_train = full_sub_x[:int(len(full_sub_x)*0.9)]\n",
    "new_sub_y_train = full_sub_y[:int(len(full_sub_y)*0.9)]\n",
    "\n",
    "new_main_x_test = new_main_x_test.rename(\"question\") \n",
    "new_main_x_train = new_main_x_train.rename(\"question\") \n",
    "new_main_y_test = new_main_y_test.rename(\"main_category\") \n",
    "new_main_y_train = new_main_y_train.rename(\"main_category\") \n",
    "new_sub_x_test = new_sub_x_test.rename(\"question\") \n",
    "new_sub_x_train = new_sub_x_train.rename(\"question\") \n",
    "new_sub_y_test = new_sub_y_test.rename(\"sub_category\") \n",
    "new_sub_y_train = new_sub_y_train.rename(\"sub_category\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sub_y_train = new_sub_y_train.astype('int64')\n",
    "new_main_y_train = new_main_y_train.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_main_train_dataset = WordData(list(new_main_x_train), list(new_main_y_train), word2index)\n",
    "new_main_train_loader = DataLoader(new_main_train_dataset, batch_size=64)\n",
    "\n",
    "new_sub_train_dataset = WordData(list(new_sub_x_train), list(new_sub_y_train), word2index)\n",
    "new_sub_train_loader = DataLoader(new_sub_train_dataset, batch_size=64)\n",
    "\n",
    "new_main_validation_dataset = WordData(list(new_main_x_test), list(new_main_y_test), word2index)\n",
    "new_main_validation_loader = DataLoader(new_main_validation_dataset, batch_size=64)\n",
    "\n",
    "new_sub_validation_dataset = WordData(list(new_sub_x_test), list(new_sub_y_test), word2index)\n",
    "new_sub_validation_loader = DataLoader(new_sub_validation_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_num_classes = 28\n",
    "sub_num_classes = 209\n",
    "main_mode = DeepAverageNetwork(embedding_matrix=vectors, n_classes=main_num_classes)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "sub_mode = DeepAverageNetwork(embedding_matrix=vectors, n_classes=sub_num_classes)\n",
    "sub_mode = sub_mode.to(device)\n",
    "optimizer2 = optim.Adam(sub_mode.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')    \n",
    "\n",
    "\n",
    "\n",
    "main_mode = main_mode.to(device)\n",
    "\n",
    "optimizer1 = optim.Adam(main_mode.parameters(), lr=0.01)\n",
    "\n",
    "criterion = criterion.to(device)\n",
    "# Training\n",
    "\n",
    "epochs = 10\n",
    "main_losses = []\n",
    "sub_losses = []\n",
    "best_test_loss = 10.\n",
    "\n",
    "main_test_f1, sub_test_f1 = [], []\n",
    "\n",
    "for n_epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    main_train_losses = []\n",
    "    main_test_losses = []\n",
    "    main_test_targets = []\n",
    "    main_test_pred_class = []\n",
    "    sub_train_losses = []\n",
    "    sub_test_losses = []\n",
    "    sub_test_targets = []\n",
    "    sub_test_pred_class = []\n",
    "    \n",
    "    sub_unsu_targets = []\n",
    "    sub_unsu_pred_class = []\n",
    "    main_unsu_targets = []\n",
    "    main_unsu_pred_class = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    main_mode.train()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for x, y in new_main_train_loader:\n",
    "        \n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer1.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred = main_mode(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "        main_train_losses.append(loss.item())\n",
    "        main_losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    main_mode.eval()\n",
    "    \n",
    "    \n",
    "    for x, y in new_main_validation_loader:\n",
    "        \n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "           \n",
    "            pred = main_mode(x)\n",
    "            \n",
    "            pred = pred.cpu()\n",
    "            y = y.cpu()\n",
    "\n",
    "            main_test_targets.append(y.numpy())\n",
    "            main_test_pred_class.append(np.argmax(pred, axis=1))\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            main_test_losses.append(loss.item())\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    sub_mode.train()\n",
    "    \n",
    "    for x, y in new_sub_train_loader:\n",
    "        \n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        pred = sub_mode(x)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer2.step()\n",
    "        \n",
    "        sub_train_losses.append(loss.item())\n",
    "        sub_losses.append(loss.item())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    sub_mode.eval()\n",
    "    \n",
    "    for x, y in new_sub_validation_loader:\n",
    "        \n",
    "        x = x.long()\n",
    "        y = y.long()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred = sub_mode(x)\n",
    "\n",
    "            pred = pred.cpu()\n",
    "            y = y.cpu()\n",
    "\n",
    "            sub_test_targets.append(y.numpy())\n",
    "            sub_test_pred_class.append(np.argmax(pred, axis=1))\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            sub_test_losses.append(loss.item())\n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    "        \n",
    "main_test_loss = np.mean(main_test_losses)\n",
    "main_test_targets = np.concatenate(main_test_targets).squeeze()\n",
    "main_test_pred_class = np.concatenate(main_test_pred_class).squeeze()\n",
    "main_f1 = f1_score(main_test_targets, main_test_pred_class, average='micro')\n",
    "main_test_f1.append(main_f1)\n",
    "sub_test_loss = np.mean(sub_test_losses)\n",
    "sub_test_targets = np.concatenate(sub_test_targets).squeeze()\n",
    "sub_test_pred_class = np.concatenate(sub_test_pred_class).squeeze()\n",
    "sub_f1 = f1_score(sub_test_targets, sub_test_pred_class, average='micro')\n",
    "sub_test_f1.append(sub_f1)\n",
    "    \n",
    "print()\n",
    "print('Main category losses: train - {:.3f}, test - {:.3f}'.format(np.mean(main_train_losses), main_test_loss))\n",
    "print('Sub category losses: train - {:.3f}, test - {:.3f}'.format(np.mean(sub_train_losses), sub_test_loss))\n",
    "print('F1 test - {:.3f}'.format(main_f1*0.7 + sub_f1*0.3))\n",
    "mean_test_loss = np.mean(main_test_losses)*0.7 + np.mean(sub_test_losses)*0.3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(total=len(main_train_loader.dataset), desc='Epoch Main {}'.format(n_epoch + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main category losses: train - 1.719, test - 1.736\n",
    "<br>Sub category losses: train - 3.306, test - 3.321\n",
    "<br>F1 test - 0.438"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple network yields a result which is slightly worse than that of the complicated network.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
